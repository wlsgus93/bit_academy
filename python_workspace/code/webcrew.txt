import requests                  # 웹 페이지의 HTML을 가져오는 모듈
from bs4 import BeautifulSoup    # HTML을 파싱하는 모듈
 
# 웹 페이지를 가져온 뒤 BeautifulSoup 객체로 만듦
response = requests.get('http://www.weather.go.kr/weather/observation/currentweather.jsp')
soup = BeautifulSoup(response.content, 'html.parser')
 
table = soup.find('table', { 'class': 'table_develop3' })   
data = []                         
i = 0
for tr in table.find_all('tr'):      
    tds = list(tr.find_all('td'))    
                                    
    for td in tds:                  
        if td.find('a'):            
            point = td.find('a').text    
            temperature = tds[5].text    
            humidity = tds[9].text     
            data.append([point, temperature, humidity])  
 
data   


with open('weather.csv', 'w') as file:   
    file.write('point,temperature,humidity\n')                 
    for i in data:                                              
        file.write('{0},{1},{2}\n'.format(i[0], i[1], i[2])) 



import os, re, csv
import requests 
#import urllib.request  
import urllib.request as ur
from bs4 import BeautifulSoup as bs
url='https://news.daum.net'
soup = bs(ur.urlopen(url).read(), 'html.parser')
soup

# tt = soup.find_all('div',{'class':'item_issue'})
# for title in tt:
#     print(title.get_text())

# ta =  soup.find_all('a')[:5]
# for hhh in ta:
#     print(hhh.get('href'))

# ttt = soup.find_all('div',{'class':'item_issue'})
# for i in ttt:
#     print(i.find_all('a')[0].get('href'))

# 링크를 파일로 저장
# f= open('links.txt','w')
# for i in soup.find_all('div',{"class":"item_issue"}):
#     f.write(i.find_all('a')[0].get('href')+'\n' )
# f.close()


#기사 제목과 내용 추출
# covid = 'https://news.v.daum.net/v/20200917224339399'
# soup2 = bs(ur.urlopen(covid).read(), 'html.parser')

# for i in soup2.find_all('p'):
#     print(i.text)


# 헤드라인 꺼내보기
# headline = soup2.find_all('h3', {'class':'tit_view'})
# print(headline[0].text)


#모든기사제목과 본문추출
# headnews='https://news.daum.net/'
# soup3 = bs(ur.urlopen(headnews).read(), 'html.parser')

# headline = soup3.find_all('div', {"class":"feature_home"})

# for i in headline:
#     print('#####'+ i.text, '\n')
#     soup4 = bs(ur.urlopen(i.find_all('a')[0].get('href')).read(), 'html.parser')
#     for j in soup4.find_all('p'):
#         print('======' + j.text)

